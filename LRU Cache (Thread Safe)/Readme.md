# Thread-Safe LRU Cache in C++

A **thread-safe Least Recently Used (LRU) Cache** implementation in **C++**, designed for high performance, concurrency, and clean architecture. This project includes comprehensive documentation, real-world usage examples, and unit tests covering edge cases and concurrent access.

---

## ğŸ“Œ Overview

An **LRU Cache** is a data structure that stores a limited number of key-value pairs and **automatically evicts the least recently used item** when the cache exceeds its capacity.

This implementation is ideal for:
- In-memory caching
- Backend systems
- Performance-critical applications
- Interview-ready system design examples

---

## âœ¨ Features

- ğŸ”’ **Thread-Safe**
  - Uses `std::shared_mutex` to allow safe concurrent access
- âš¡ **O(1) Time Complexity**
  - `get`, `put`, and `remove` operations run in constant time
- â™»ï¸ **Automatic LRU Eviction**
  - Least recently used entry is evicted when capacity is reached
- ğŸ“¦ **Customizable Capacity**
  - Set cache size at initialization
- ğŸ§© **Generic Implementation**
  - Supports any key-value types using C++ Templates
- ğŸ§ª **Comprehensive Tests**
  - Unit tests + edge cases + concurrency tests

---

## ğŸ—ï¸ Architecture

The cache is implemented using a combination of:

| Component | Purpose |
|---------|--------|
| `std::unordered_map<K, Node>` | O(1) key lookup |
| `std::shared_ptr<Node>` linked list | Maintains access order |
| `std::shared_mutex` | Thread-safe concurrent access |

### Why This Design?
- `unordered_map` gives fast lookup
- Shared pointer linked list enables O(1) insertion and removal
- `shared_mutex` ensures safe multi-threaded access

---

## ğŸ“Š System Architecture Diagrams

### 1. Data Structure Layout

```
LRUCache Object:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LRUCache<K, V>                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Attributes:                                 â”‚
â”‚  â”œâ”€ capacity_: int = 5                       â”‚
â”‚  â”œâ”€ lock_: std::shared_mutex                 â”‚
â”‚  â”œâ”€ map_: unordered_map<K, Node*>           â”‚
â”‚  â”œâ”€ head_: Node*  â”€â”€â”                        â”‚
â”‚  â””â”€ tail_: Node*  â”€â”€â”¤                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
Linked List Structure (Doubly-Linked):
â”Œâ”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ HEAD â”‚â—„â”€â”€â”€â–ºâ”‚ k:1  â”‚â—„â”€â”€â”€â–ºâ”‚ k:5  â”‚â—„â”€â”€â”€â–ºâ”‚ k:3  â”‚â—„â”€â”€â”€â–ºâ”‚ TAIL â”‚
â””â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”˜
(sentinel)   (oldest)                            (newest)
                                                  [MRU end]

HashMap Index:
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ k:1 â”‚ k:5 â”‚ k:3 â”‚ ... â”‚ ... â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ â†“   â”‚ â†“   â”‚ â†“   â”‚     â”‚     â”‚  Points to nodes in linked list
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
  O(1)  O(1)  O(1)  lookup

MRU (Most Recently Used) â”€â”€â†’ Newest entries
LRU (Least Recently Used) â”€â”€â†’ Oldest entries (eviction candidates)
```

### 2. Operation Flow Diagrams

#### GET Operation Flow
```
Thread calls: cache.get("key1")
â”‚
â”œâ”€â–º Acquire SHARED LOCK (read_lock)
â”‚   â”‚
â”‚   â”œâ”€â–º HashMap lookup: map_.find("key1")
â”‚   â”‚   â”œâ”€â–º Found? Continue
â”‚   â”‚   â””â”€â–º Not found? Return nullptr (release lock)
â”‚   â”‚
â”‚   â”œâ”€â–º Get node reference
â”‚   â”‚
â”‚   â””â”€â–º Release SHARED LOCK
â”‚       â”‚
â”‚       â–¼
â”œâ”€â–º Acquire UNIQUE LOCK (write_lock)  [Now exclusive]
â”‚   â”‚
â”‚   â”œâ”€â–º removeNode(node)  [Unlink from current position]
â”‚   â”‚
â”‚   â”œâ”€â–º addNodeToEnd(node)  [Move to end (MRU)]
â”‚   â”‚
â”‚   â””â”€â–º Release UNIQUE LOCK
â”‚
â””â”€â–º Return value (copy)
```

#### PUT Operation Flow
```
Thread calls: cache.put("key2", value)
â”‚
â”œâ”€â–º Acquire UNIQUE LOCK (exclusive write access)
â”‚   â”‚
â”‚   â”œâ”€â–º Check if key exists in map_
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â–º YES (Update):
â”‚   â”‚   â”‚   â”œâ”€â–º Update value
â”‚   â”‚   â”‚   â”œâ”€â–º removeNode(node)
â”‚   â”‚   â”‚   â””â”€â–º addNodeToEnd(node)  [Mark as MRU]
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â–º NO (Insert):
â”‚   â”‚       â”œâ”€â–º Check if size >= capacity
â”‚   â”‚       â”‚   â”‚
â”‚   â”‚       â”‚   â””â”€â–º YES: Evict LRU
â”‚   â”‚       â”‚       â”œâ”€â–º lru_node = head_.next
â”‚   â”‚       â”‚       â”œâ”€â–º removeNode(lru_node)
â”‚   â”‚       â”‚       â””â”€â–º map_.erase(lru_node.key)
â”‚   â”‚       â”‚
â”‚   â”‚       â”œâ”€â–º Create new_node
â”‚   â”‚       â”œâ”€â–º addNodeToEnd(new_node)
â”‚   â”‚       â””â”€â–º map_["key2"] = new_node
â”‚   â”‚
â”‚   â””â”€â–º Release UNIQUE LOCK
â”‚
â””â”€â–º Return
```

### 3. Concurrent Access Scenarios

#### Scenario A: Multiple Readers (Safe âœ…)
```
Time â†’
0ms:  Thread1 calls get("a")
      â”‚
10ms: â”œâ”€ acquires SHARED LOCK â”€ holds
      â”‚
20ms: â”œâ”€ Thread2 calls get("b")
      â”‚  â”œâ”€ acquires SHARED LOCK â”€ holds (CONCURRENT! âœ…)
      â”‚
30ms: â”œâ”€ Thread3 calls get("c")
      â”‚  â”œâ”€ acquires SHARED LOCK â”€ holds (CONCURRENT! âœ…)
      â”‚
40ms: â”œâ”€ Thread1 releases SHARED LOCK
      â”‚
50ms: â”œâ”€ Thread2 releases SHARED LOCK
      â”‚
60ms: â””â”€ Thread3 releases SHARED LOCK

Result: All 3 readers operated simultaneously on cache
        No waiting, maximum parallelism âœ…
```

#### Scenario B: Reader + Writer (Safe, Serialized)
```
Time â†’
0ms:  Thread1 (Reader) calls get("a")
      â”‚
10ms: â”œâ”€ acquires SHARED LOCK â”€ holds
      â”‚
20ms: â”œâ”€ Thread2 (Writer) calls put("x", val)
      â”‚  â””â”€ BLOCKED waiting for exclusive lock âŒ
      â”‚
30ms: â”œâ”€ Thread1 completes
      â”‚  â”œâ”€ releases SHARED LOCK
      â”‚  â”‚
      â”‚  â””â”€ Thread2 acquires UNIQUE LOCK (now exclusive)
      â”‚
40ms: â”œâ”€ Thread2 modifies cache
      â”‚
50ms: â””â”€ Thread2 releases UNIQUE LOCK

Result: Writer waited for all readers to finish
        No race conditions âœ…
```

#### Scenario C: Multiple Writers (Safe, Serialized)
```
Time â†’
0ms:  Thread1 (Writer) calls put("a", val1)
      â”‚
10ms: â”œâ”€ acquires UNIQUE LOCK (exclusive)
      â”‚
20ms: â”œâ”€ Thread2 (Writer) calls put("b", val2)
      â”‚  â””â”€ BLOCKED waiting for exclusive lock âŒ
      â”‚
30ms: â”œâ”€ Thread1 releases UNIQUE LOCK
      â”‚
40ms: â”œâ”€ Thread2 acquires UNIQUE LOCK (exclusive)
      â”‚
50ms: â”œâ”€ Thread2 modifies cache
      â”‚
60ms: â””â”€ Thread2 releases UNIQUE LOCK

Result: Writers serialized (one at a time)
        State consistency guaranteed âœ…
```

### 4. Lock State Machine

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   NO LOCK HELD          â”‚
                    â”‚   (Idle State)          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚                    â”‚
                 â–¼                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  SHARED LOCK     â”‚  â”‚  UNIQUE LOCK     â”‚
        â”‚  (Read Mode)     â”‚  â”‚  (Write Mode)    â”‚
        â”‚                  â”‚  â”‚                  â”‚
        â”‚ âœ“ Multiple       â”‚  â”‚ âœ— Exclusive      â”‚
        â”‚   readers        â”‚  â”‚                  â”‚
        â”‚ âœ— No writers     â”‚  â”‚ âœ— No readers     â”‚
        â”‚ âœ— No upgrades    â”‚  â”‚ âœ— No other       â”‚
        â”‚                  â”‚  â”‚    writers       â”‚
        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                     â”‚
             â”‚ (release)           â”‚ (release)
             â”‚                     â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   NO LOCK HELD          â”‚
                â”‚   (Idle State)          â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Shared Lock Holders: Can coexist
Unique Lock Holder: Blocks all, blocked by all
```

### 5. Thread Safety Verification Checklist

| Criterion | Status | Evidence |
|-----------|--------|----------|
| **No Data Races** | âœ… PASS | All shared state (`map_`, `head_`, `tail_`) protected by `lock_` |
| **No Race Conditions** | âœ… PASS | Operations are atomic from caller's perspective (with locks) |
| **No Deadlocks** | âœ… PASS | Single lock hierarchy, consistent acquisition order |
| **No Livelocks** | âœ… PASS | No infinite retry loops; lock-based (not spin-based) |
| **Exception Safe** | âœ… PASS | RAII pattern with `std::unique_lock` and `std::shared_lock` |
| **Memory Safe** | âœ… PASS | `std::shared_ptr` prevents use-after-free |
| **ABA Problem Free** | âœ… PASS | Pointer-based nodes with shared_ptr maintain identity |
| **Starvation Free** | âš ï¸ CAUTION | Writers may starve under continuous readers |

**Overall: âœ… THREAD-SAFE for concurrent read/write operations**

### 6. Starvation Analysis

**Reader Starvation**: âœ… NOT POSSIBLE
- Writers acquire unique lock, blocking new readers

**Writer Starvation**: âš ï¸ POSSIBLE
- Continuous readers can prevent writers from proceeding
- **Mitigation**: Use lock with writer preference if critical
- **Alternative**: Monitor writer wait times and apply backpressure

**Example Writer Starvation**:
```
Reader 1: Acquire â†’ Hold â†’ Release
  â†“ (immediately)
Reader 2: Acquire â†’ Hold â†’ Release
  â†“ (immediately)
Reader 3: Acquire â†’ Hold â†’ Release
  â†“ (continuously)
Writer: WAITING... WAITING... WAITING... âŒ

Solution: Implement writer-preferred or fair locking strategy
```

---

## ğŸš€ Usage

```cpp
// Create a cache with capacity of 5
LRUCache<std::string, std::string> cache(5);

// Put values
cache.put("key1", "value1");
cache.put("key2", "value2");

// Get value
auto value = cache.get("key1");
if (value) {
    std::cout << *value << std::endl;
}

// Check existence
if (cache.containsKey("key1")) {
    std::cout << "Key found in cache" << std::endl;
}

// Remove a key
cache.remove("key1");

// Cache size
int size = cache.size();

// Clear cache
cache.clear();

```
### ğŸ“š API Documentation

### Public Methods

| Method                | Description                                   |
| --------------------- | --------------------------------------------- |
| `put(K key, V value)` | Adds or updates a key-value pair              |
| `get(K key)`          | Retrieves value and marks it as recently used |
| `remove(K key)`       | Removes a specific key                        |
| `clear()`             | Clears the entire cache                       |
| `containsKey(K key)`  | Checks if key exists                          |
| `size()`              | Returns current cache size                    |
| `isEmpty()`           | Checks if cache is empty                      |


### â±ï¸ Performance Characteristics

| Operation | Time Complexity | Space Complexity |
| --------- | --------------- | ---------------- |
| Get       | O(1)            | O(1)             |
| Put       | O(1)            | O(1)             |
| Remove    | O(1)            | O(1)             |
| Clear     | O(n)            | O(1)             |

### ğŸ” Thread Safety Analysis

**âœ… YES - This system IS thread-safe**

#### Thread Safety Mechanism

The LRU Cache uses `std::shared_mutex` to provide robust thread-safe operations:

| Scenario | Lock Type | Behavior |
|----------|-----------|----------|
| Multiple reads (`get`, `containsKey`, `size`, `isEmpty`) | Shared Lock | âœ… All readers proceed concurrently |
| Single write (`put`, `remove`, `clear`) | Unique Lock | ğŸ”’ Exclusive access, readers blocked |
| Read after write lock upgrade | Upgrade Pattern | âœ… Deadlock-free |

#### Thread-Safety Guarantees

1. **No Data Races**: All access to `map_`, `head_`, `tail_` is protected by `lock_`
2. **No Race Conditions**: State modifications are atomic from the caller's perspective
3. **Deadlock-Free**: Lock acquisition order is consistent (always `lock_` first)
4. **ABA Problem Prevention**: Using `shared_ptr` prevents use-after-free issues

#### Lock-Based Concurrency Pattern

```
Read Operations (get, containsKey, size, isEmpty):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Acquire Shared Lock    â”‚ â† Multiple threads can hold simultaneously
  â”‚  (std::shared_lock)     â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚  Perform Read           â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚  Release Shared Lock    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘
    CAN OVERLAP
         â†‘
    Multiple readers
    
Write Operations (put, remove, clear):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Acquire Unique Lock    â”‚ â† Only ONE thread at a time
  â”‚  (std::unique_lock)     â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚  Perform Write          â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚  Release Unique Lock    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘
   NO OVERLAP
         â†‘
   Exclusive access
```

#### Thread Safety in Practice

**Safe Scenario:**
```
Thread 1: cache.get("key1")       â† Holds shared lock
Thread 2: cache.get("key2")       â† Also holds shared lock (same shared lock)
Thread 3: cache.put("key3", val)  â† BLOCKED, waiting for exclusive lock
Thread 1 & 2: Release locks       â† Thread 3 proceeds with exclusive lock
```

**Prevented Race Condition Example:**
```
WITHOUT Lock (RACE CONDITION):
  Thread 1: Read "size" = 2
  Thread 2: Write - evict oldest
  Thread 1: Write - assume space available (WRONG! Cache full)
  
WITH Lock (NO RACE):
  Thread 1: Acquire shared lock â†’ Read "size" = 2
  Thread 2: WAITS for shared locks to release
  Thread 1: Release shared lock
  Thread 2: Acquire exclusive lock â†’ Modify cache safely
```

#### Critical Sections Protected

| State | Lock | Operations |
|-------|------|-----------|
| `map_` (HashMap) | `lock_` | All read/write operations |
| `head_` (Linked List Sentinel) | `lock_` | List traversal and modifications |
| `tail_` (Linked List Sentinel) | `lock_` | List traversal and modifications |
| `capacity_` | None (constant) | Read-only, no lock needed |

#### Potential Issues & Mitigations

| Issue | Severity | Mitigation |
|-------|----------|-----------|
| Lock contention under high load | Medium | Monitor with performance profiling |
| Priority inversion (low-priority writer blocks high-priority reader) | Low | Use lock-free atomics if critical |
| Exception safety during writes | Low | RAII ensures locks released on exception |

Suitable for multi-threaded C++ backend services.
### ğŸ“ Project Structure
    LRU-Cache/
    â”œâ”€â”€ LRUCache.h
    â”œâ”€â”€ LRUCacheTest.cpp
    â”œâ”€â”€ README.md
    â”œâ”€â”€ LICENSE
    â””â”€â”€ Readme.md




### ğŸ§ª Example Scenarios
### 1ï¸âƒ£ Basic Cache Usage
    LRUCache<int, int> cache(3);
    cache.put(1, 1);
    cache.put(2, 2);
    cache.put(3, 3);
    
    Cache: {1=1, 2=2, 3=3}
    
    cache.put(4, 4); // Evicts 1
    Cache: {2=2, 3=3, 4=4}

### 2ï¸âƒ£ Access Updates Order
    LRUCache<int, int> cache(3);
    cache.put(1, 1);
    cache.put(2, 2);
    cache.put(3, 3);
    
    // Cache: {1=1, 2=2, 3=3}
    
    cache.put(4, 4); // Evicts 1
    // Cache: {2=2, 3=3, 4=4}

### ğŸ§  Design Patterns Used

- Generic Programming

    - C++ Templates for type safety

- Synchronization

    - Shared mutex locking for concurrency

- Composite Data Structures

    - unordered_map + Doubly Linked List (via shared_ptr)


### ğŸš§ Limitations & Future Enhancements
### Current Limitations

-   Fixed capacity only

-   No persistence

### Planned Enhancements

-   â±ï¸ Time-based expiration (TTL)

-   ğŸ”„ Custom eviction policies

-   ğŸ“Š Cache statistics (hit/miss ratio)
